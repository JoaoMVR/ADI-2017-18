{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratory 6: Reinforcement learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. The windy gridworld domain\n",
    "\n",
    "Consider the larger version of the windy gridworld domain depicted in the figure below.\n",
    "\n",
    "<img src=\"windy.png\" width=\"400px\">\n",
    "\n",
    "In it, a boat must navigate a 7 &times; 10 gridworld, to reach the goal cell, marked with _G_. There is a crosswind upward through the middle of the grid, in the direction indicated by the gray arrows. The boat has available the standard four actions -- _Up_, _Down_, _Left_ and _Right_. In the region affected by the wind, however, the resulting next state is shifted upward as a consequence of the crosswind, the strength of which varies from column to column. The strength of the wind is given below each column, and corresponds to the number of cells that the movement is shifted upward. For example, if the boat is one cell to the right of the goal, then the action _Left_ takes you to the cell just above the goal.\n",
    "\n",
    "The agent pays a cost of 1 in every step before reaching the goal. The problem can be described as an MDP $(\\mathcal{X},\\mathcal{A},\\mathbf{P},c,\\gamma)$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "# Problem specific parameters\n",
    "WIND = (0, 0, 0, 1, 1, 1, 2, 2, 1, 0)\n",
    "nrows = 7\n",
    "ncols = 10\n",
    "init = [3, 0]\n",
    "goal = [3, 7]\n",
    "\n",
    "# States\n",
    "X = [[x, y] for x in range(nrows) for y in range(ncols)]\n",
    "nX = len(X)\n",
    "\n",
    "# Actions\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "nA = len(A)\n",
    "\n",
    "# Transition probabilities\n",
    "P = dict()\n",
    "P['U'] = np.zeros((nX, nX))\n",
    "P['D'] = np.zeros((nX, nX))\n",
    "P['L'] = np.zeros((nX, nX))\n",
    "P['R'] = np.zeros((nX, nX))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    x = X[i]\n",
    "    y = dict()\n",
    "    \n",
    "    y['U'] = [x[0] - WIND[x[1]] - 1, x[1]]\n",
    "    y['D'] = [x[0] - WIND[x[1]] + 1, x[1]]\n",
    "    y['L'] = [x[0] - WIND[x[1]], x[1] - 1]\n",
    "    y['R'] = [x[0] - WIND[x[1]], x[1] + 1]\n",
    "    \n",
    "    for k in y:\n",
    "        y[k][0] = max(min(y[k][0], nrows - 1), 0)\n",
    "        y[k][1] = max(min(y[k][1], ncols - 1), 0)\n",
    "        j = X.index(y[k])\n",
    "        P[k][i, j] = 1\n",
    "\n",
    "c = np.ones((nX, nA))\n",
    "c[X.index(goal), :] = 0\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# -- Pretty print\n",
    "\n",
    "print('\\n- MDP problem specification: -\\n')\n",
    "\n",
    "print('States:')\n",
    "print(np.array(X))\n",
    "\n",
    "print('\\nActions:')\n",
    "print(A)\n",
    "\n",
    "print('\\nTransition probabilities:')\n",
    "for a in A:\n",
    "    print('Action', a)\n",
    "    print(P[a])\n",
    "    \n",
    "print('\\ncost:')\n",
    "print(c)\n",
    "#for i, c1 in enumerate(c):\n",
    "#    print(i, \"->\", c1)\n",
    "    \n",
    "print('\\nStart state:', init)\n",
    "print('\\nGoal state:', goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Compute the optimal _Q_-function for the MDP defined above using value iteration. As your stopping condition, use an error between iterations smaller than `1e-8`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e = pow(10, -8)\n",
    "\n",
    "err = 1\n",
    "J = np.zeros((nX, 1))\n",
    "\n",
    "cU = c[:, 0].reshape(nX, 1)\n",
    "cD = c[:, 1].reshape(nX, 1)\n",
    "cL = c[:, 2].reshape(nX, 1)\n",
    "cR = c[:, 3].reshape(nX, 1)\n",
    "\n",
    "qU = np.zeros((nX, 1))\n",
    "qD = np.zeros((nX, 1))\n",
    "qL = np.zeros((nX, 1))\n",
    "qR = np.zeros((nX, 1))\n",
    "\n",
    "while (err > 1e-8):\n",
    "    qU = cU + gamma * P['U'].dot(J)\n",
    "    qD = cD + gamma * P['D'].dot(J)\n",
    "    qL = cL + gamma * P['L'].dot(J)\n",
    "    qR = cR + gamma * P['R'].dot(J)\n",
    "    newJ = np.min((qU, qD, qL, qR), axis = 0)\n",
    "    err = np.linalg.norm(newJ - J)\n",
    "    J = newJ\n",
    "    i = i + 1\n",
    "\n",
    "optimal_Q = np.concatenate((qU,qD,qL,qR), axis=1)\n",
    "\n",
    "print(\"Q-function: \\n\", optimal_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Write down a Python function that, given a Q-function $Q$ and a state $x$, selects a random action using the $\\epsilon$-greedy policy obtained from $Q$ for state $x$. Your function should receive an optional parameter, corresponding to $\\epsilon$, with default value of 0.1. \n",
    "\n",
    "**Note:** In the case of two actions with the same value, your $\\epsilon$-greedy policy should randomize between the two.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import uniform, randint\n",
    "\n",
    "def greedy(q_function, x, epsilon = 0.1):\n",
    "    q_line = q_function[X.index(x)]\n",
    "    rand = uniform(0, 1)\n",
    "    max_action_index = np.argmax(q_line)\n",
    "    random_action = randint(0, len(q_line) - 1)\n",
    "    action = A[max_action_index] if rand <= epsilon else A[random_action]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Model-based learning\n",
    "\n",
    "You will now run the model-based learning algorithm discussed in class, and evaluate its learning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "Run the model-based reinforcement learning algorithm discussed in class to compute $Q^*$ for 100,000 iterations. Initialize each transition probability matrix as the identity and the cost function as all-zeros. Use an $\\epsilon$-greedy policy with $\\epsilon=0.1$ (use the function from Activity 2). Note that, at each step,\n",
    "\n",
    "* You will need to select an action according to the $\\epsilon$-greedy policy;\n",
    "* The state and action, you will then compute the cost and generate the next state; \n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update. \n",
    "* When updating the components $(x,a)$ of the model, use the step-size\n",
    "\n",
    "$$\\alpha_t=\\frac{1}{N_t(x,a)+1},$$\n",
    "\n",
    "where $N_t(x,a)$ is the number of visits to the pair $(x,a)$ up to time step $t$.\n",
    "\n",
    "In order to ensure that your algorithm visits every state and action a sufficient number of times, after the boat reaches the goal cell, make one further step, the corresponding update, and then reset the position of the boat to a random state in the environment.\n",
    "\n",
    "Plot the norm $\\|Q^*-Q^{(k)}\\|$ every 500 iterations of your method, where $Q^*$ is the optimal _Q_~function computed in Activity 1.\n",
    "\n",
    "**Note:** The simulation may take a bit. Don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new_P = dict()\n",
    "new_P['U'] = np.identity(nX)\n",
    "new_P['D'] = np.identity(nX)\n",
    "new_P['L'] = np.identity(nX)\n",
    "new_P['R'] = np.identity(nX)\n",
    "\n",
    "new_C = np.zeros((nX,nA))\n",
    "\n",
    "new_Q = np.zeros((nX, nA))\n",
    "\n",
    "N = np.zeros((nX, nA))\n",
    "\n",
    "current_X = init\n",
    "Q_difference_y = []\n",
    "Q_difference_x = []\n",
    "random_init = False\n",
    "\n",
    "for i in range(100000):\n",
    "    #select action\n",
    "    current_action = greedy(new_Q, current_X)\n",
    "    \n",
    "    #apply action -> new state\n",
    "    #it's deterministic, but let's do it like this\n",
    "    pX = P[current_action][X.index(current_X)]\n",
    "    new_X = X[np.random.choice(np.arange(nX), 1, p = pX)[0]]\n",
    "    \n",
    "    #indexes\n",
    "    i_X = X.index(current_X)\n",
    "    i_new_X = X.index(new_X)\n",
    "    i_A = A.index(current_action)\n",
    "    \n",
    "    #new step\n",
    "    N[i_X, i_A] = N[i_X, i_A] + 1\n",
    "    step = 1 / (N[i_X, i_A] + 1)\n",
    "    \n",
    "    #update cost\n",
    "    new_C[i_X, i_A] = new_C[i_X, i_A] + step*(c[i_X, i_A] - new_C[i_X, i_A])\n",
    "        \n",
    "    #update P\n",
    "    p_sum = np.zeros(nX)\n",
    "    p_sum[i_new_X] = 1\n",
    "    new_P[current_action][i_X] = new_P[current_action][i_X] + step*( p_sum - new_P[current_action][i_X])\n",
    "    new_P[current_action][i_X] = new_P[current_action][i_X] / np.sum(new_P[current_action][i_X])\n",
    "    \n",
    "    #update Q\n",
    "    new_Q[i_X, i_A] = new_C[i_X, i_A] + new_P[current_action][i_X].dot( np.min(new_Q, axis = 1) )\n",
    "    \n",
    "    #update state\n",
    "    if (random_init):\n",
    "        random_init = False\n",
    "        new_X = X[np.random.randint(nX)]\n",
    "    \n",
    "    elif (current_X == goal):\n",
    "        random_init = True\n",
    "\n",
    "    current_X = new_X\n",
    "    \n",
    "    #plot ya later\n",
    "    if (i%500 == 0):\n",
    "        Q_difference_x.append(i)\n",
    "        Q_difference_y.append(np.linalg.norm(optimal_Q - new_Q))\n",
    "        \n",
    "#print(Q_difference_y)\n",
    "#print(new_Q)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Q_difference_x, Q_difference_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Temporal-difference learning\n",
    "\n",
    "You will now run both Q-learning and SARSA, and compare their learning performance with that of the model-based method just studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Repeat Activity 3 but using the _Q_-learning algorithm with a learning rate $\\alpha=0.3$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Repeat Activity 4 but using the SARSA algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "Discuss the differences observed between the performance of the three methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "_Add your discussion here._"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
