{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratory 5: Supervised learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. The IRIS dataset\n",
    "\n",
    "The Iris flower data set is a data set describing the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the GaspÃ© Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (_Iris setosa_, _Iris virginica_ and _Iris versicolor_). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres.\n",
    "\n",
    "In your work, you will use only two of the four features and consider only two of the three species of Iris.\n",
    "\n",
    "---\n",
    "\n",
    "We start by loading the dataset and plotting the two classes of points that we wish to discriminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "#print(iris.DESCR)\n",
    "\n",
    "X = iris.data[50:,2:]\n",
    "a = iris.target[50:]\n",
    "\n",
    "# Plot data\n",
    "plt.plot(X[:50, 0], X[:50, 1], 'bx', label='Versicolour')\n",
    "plt.plot(X[50:, 0], X[50:, 1], 'ro', label='Virginica')\n",
    "plt.xlabel('Petal length (cm)')\n",
    "plt.ylabel('Petal width (cm)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Train a logistic regression classifier in Python using Newton-Raphson's method. The method is described by the update:\n",
    "\n",
    "$$\\mathbf{w}^{(k+1)}\\leftarrow\\mathbf{w}^{(k)}-\\mathbf{H}^{-1}\\mathbf{g},$$\n",
    "\n",
    "where $\\mathbf{H}$ and $\\mathbf{g}$ are the _Hessian matrix_ and _gradient vector_ that you computed in your homework. Therefore, to train the classifier you should write a cycle that repeatedly updates the parameter vector according to the rule above until the difference between two iterations is sufficiently small (e.g., smaller than $10^{-5}$).\n",
    "\n",
    "Print the resulting parameters and plot the decision boundary over the data points. Make sure that:\n",
    "\n",
    "1. You augment your data pointa with an extra coordinate that is always 1\n",
    "2. The output vector takes only values 0 and 1\n",
    "3. You initialize your parameters to zero.\n",
    "\n",
    "**Note:** Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def calc_new_pi(w, x):\n",
    "    return 1 / (1 + np.exp(- np.dot(x, w)))\n",
    "\n",
    "def calc_new_gradient(x, a, p):\n",
    "    return np.dot((a - p[:,0]), x)\n",
    "\n",
    "def calc_new_hessian(pi_arg):\n",
    "    return -Xn.T.dot(np.diag(pi_arg[:,0] * (1 - pi_arg[:,0])).dot(Xn))\n",
    "\n",
    "def calc_new_parameters(w_old):\n",
    "    pi = calc_new_pi(w_old, Xn)\n",
    "    h = calc_new_hessian(pi)\n",
    "    g = calc_new_gradient(Xn, an, pi)\n",
    "    w_new = w_old - np.linalg.inv(h).dot(g[:, None])\n",
    "    return pi, h, g, w_new\n",
    "\n",
    "def assert_different(w_o, w_n):\n",
    "    err = pow(10, -5)\n",
    "    distance = np.linalg.norm(w_o - w_n)\n",
    "    return distance >= err\n",
    "\n",
    "an = a - 1\n",
    "Xn = np.append(X, np.ones((100,1)),  axis = 1)\n",
    "#print(Xn)\n",
    "w = np.zeros((3,1))\n",
    "pi = calc_new_pi(w, Xn)\n",
    "g = calc_new_gradient(Xn, an, pi)\n",
    "#print(\"\\n\\n Gradient:\",g)\n",
    "h = calc_new_hessian(pi)\n",
    "#print(\"\\n\\n Hessian: \",h)\n",
    "\n",
    "w_old = w\n",
    "w_new = w - np.linalg.inv(h).dot(g[:, None])\n",
    "iterations = 0\n",
    "while(assert_different(w_old, w_new)):\n",
    "    w_old = w_new\n",
    "    pi, h, g, w_new = calc_new_parameters(w_old)\n",
    "    #print(w_new)\n",
    "\n",
    "w = w_new\n",
    "\n",
    "print(\"pi: \\n\", pi)\n",
    "print(\"\\ngradient: \\n\", g)\n",
    "print(\"\\nhessian: \\n\", h)\n",
    "print(\"\\nw: \\n\", w)\n",
    "    \n",
    "Xn = Xn[:,:2]\n",
    "\n",
    "condition0 = pi < 0.5\n",
    "X0i = np.extract(condition0, np.arange(100))\n",
    "X0 = Xn[X0i, :]\n",
    "condition1 = np.invert(condition0)\n",
    "X1i = np.extract(condition1, np.arange(100))\n",
    "X1 = Xn[X1i, :]\n",
    "#Xlabel = condition1.astype(int)\n",
    "\n",
    "#getting the decision boundary: y = mx + b\n",
    "#for x = 0\n",
    "x1 = 0\n",
    "y1 = -w[2]/w[1]\n",
    "#for y = 0\n",
    "x2 = -w[2]/w[0]\n",
    "y2 = 0\n",
    "m = (y1 - y2)/(x1 - x2)\n",
    "b = y1\n",
    "\n",
    "xValues = np.arange(2.5, 7.5, 0.5)\n",
    "yValues = m*xValues + b\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bx', label='Versicolour')\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', label='Virginica')\n",
    "plt.plot(xValues, yValues)\n",
    "plt.xlabel('Petal length (cm)')\n",
    "plt.ylabel('Petal width (cm)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Compare your classifier from Activity 1 with the logistic regression classifier implemented `sci-kit learn`. The code block below already loads and constructs a logistic regression model. \n",
    "\n",
    "To compare you must first fit the model to the data from Activity 1 (use the method `fit`). Next, you should build a fine grid of points $(x, y)$ in feature space (try using the `numpy` function `meshgrid`) and compute the corresponding class using the classifier (use the method `predict`). You can then use the function `plt.pcolormesh` to plot the resulting regions of decision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "model = LogisticRegression(solver='newton-cg', C=1e40)\n",
    "model.fit(Xn, an)\n",
    "\n",
    "meshX, meshY = np.meshgrid(np.arange(2.8, 7.2, 0.005), np.arange(0.8, 2.8, 0.005))\n",
    "\n",
    "meshData = np.c_[meshX.ravel(), meshY.ravel()]\n",
    "meshPred = model.predict(meshData)\n",
    "numPoints = meshPred.shape[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X0[:, 0], X0[:, 1], 'bx', label='Versicolour')\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'ro', label='Virginica')\n",
    "cmap_light = ListedColormap(['#AAAAFF', '#FFAAAA'])\n",
    "plt.pcolormesh(meshX, meshY, meshPred.reshape(meshX.shape[0],meshX.shape[1]), cmap=cmap_light)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. SPAM filtering\n",
    "\n",
    "You will now implement a spam filter, in which you will compare the results of different classifiers seen in class. In order to do so, you will first need to prepare the data for learning.\n",
    "\n",
    "The following block of code illustrates how you can use the `os` module to access a list of files in a given folder. In particular, if you uncompress the file `data.zip` your working folder, the instruction `os.listdir('data')` will return a list with the contents of folder `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir('data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uncompress the data file `data.zip` to your current folder. You will find a total of four folders, named `spam-train`, `nonspam-train`, `spam-test` and `nonsmap-test`. Each folder contains a number of text files which have been pre-processed to remove stop-words, punctuation signs, and other non-informative elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "You will now select the 3,000 most frequent words appearing in the training data. You will use the number of occurrences of these words in each e-mail as the features that describe that e-mail. The code provided already goes over all the files in the folders `*-train` and builds a dictionary (actually, a `Counter`) containing all words appearing and how often they appear. \n",
    "\n",
    "Use the information in the (`Counter`) dictionary to select the 3,000 most frequent words. Before compiling the list of most common words, make sure to remove _non-words_---for which you can use the `isalpha` method of the `str` class---and _words of length 1_. To build the list of most frequent words, you may find useful the method `most_common` of the Counter class. Make sure you end up with a _sorted list_ containing the 3,000 most frequent words. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "\n",
    "files = []\n",
    "for training_dir in ['data\\\\spam-train', 'data\\\\nonspam-train']:\n",
    "    files += [os.path.join(training_dir, f) for f in os.listdir(training_dir)]\n",
    "\n",
    "for f in files:\n",
    "    fin = open(f, 'r')\n",
    "    words += str(fin.read()).split()        \n",
    "    fin.close()\n",
    "\n",
    "d = Counter(words)\n",
    "\n",
    "\n",
    "# Filter non words and words smaller than 1 char long\n",
    "d = {k: v for k, v in d.items() if k.isalpha() and len(k)>1}\n",
    "# Filter 3000 most common\n",
    "d = Counter(d).most_common(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Each of the files in the folder `spam-train` corresponds to a datapoint $(\\mathbf{x}_n,a_n)$, where $\\mathbf{x}_n$ is a vector containing the number of times that each of the most frequent words (computed in Activity 3) appears in that file, and $a_n$ is $0$. Conversely, each of the files in the folder `nonspam-train` corresponds to a datapoint $(\\mathbf{x}_n,a_n)$, where $\\mathbf{x}_n$ is again a vector containing the number of times that each of the most frequent words appears in that file, and $a_n$ is $1$. \n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Go over the files in the aforementioned folders and create the a matrix `X` where each row $i$ is the datapoint corresponding to file $i$, and each column $j$ contains the number of times that the word $j$ appears in each of the files. Create the corresponding vector `a` of labels, where the component $i$ is 0 or 1 depending on whether file $i$ is spam or not.\n",
    "\n",
    "** Note: ** You may want to create a function that receives the name of a folder and a list of words as arguments and returns the matrix of datapoints corresponding to the files in that folder, where each datapoint is described as a vector of features and each feature corresponds to the number of occurrences of the words in the list provided.\n",
    "\n",
    "** Note 2: ** Extracting the features corresponding to the files may take a bit, so don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#vector with the 3000 most common words\n",
    "wordVec = []\n",
    "for elem in d:\n",
    "    wordVec.append(elem[0])\n",
    "\n",
    "X = np.zeros((len(files), len(wordVec)))\n",
    "\n",
    "#auxiliary\n",
    "fileWords = []\n",
    "print(\"Computing X...\")\n",
    "for i in range(len(files)):\n",
    "    fin = open(files[i], 'r')\n",
    "    fileWords += str(fin.read()).split()\n",
    "    \n",
    "    #dict with all the valid words\n",
    "    wordCounter = Counter(fileWords)\n",
    "    wordCounter = {k: v for k, v in wordCounter.items() if k.isalpha() and len(k)>1}\n",
    "    \n",
    "    #remove any word not in wordVec\n",
    "    toDelete = []\n",
    "    for k, v in wordCounter.items():\n",
    "        if k not in wordVec:\n",
    "            toDelete.append(k)\n",
    "    for w in toDelete:\n",
    "        wordCounter.pop(w)\n",
    "    \n",
    "    #filling each row\n",
    "    for j in range(len(wordVec)):\n",
    "        toDelete = \"\"\n",
    "        for k, v in wordCounter.items():\n",
    "            if k == wordVec[j]:\n",
    "                X[i, j] = v\n",
    "                toDelete = k\n",
    "                break\n",
    "        \n",
    "        #just to make things faster we delete the used keys\n",
    "        if (toDelete != \"\"):\n",
    "            wordCounter.pop(toDelete)\n",
    "    \n",
    "    fileWords = []\n",
    "    fin.close()\n",
    "    if (i % 100 == 0):\n",
    "        print(i, \"/\", len(files))\n",
    "\n",
    "print(\"X is computed\")\n",
    "\n",
    "#a vector\n",
    "for i in range(len(files)):\n",
    "    if (files[i].split(\"\\\\\")[1] != 'spam-train'):\n",
    "        break\n",
    "a = np.append(np.zeros(i), np.ones(len(files) - i), axis = 0)\n",
    "\n",
    "#this proves that it's working!\n",
    "#i = 0\n",
    "#test = np.zeros(len(d))\n",
    "#for k, v in d:\n",
    "#    test[i] = v\n",
    "#    i = i + 1\n",
    "#print(\"\\n\",np.array_equal(test, np.sum(X, axis = 0)))\n",
    "\n",
    "print(\"\\nX matrix:\\n\", X)\n",
    "print(\"\\na vector:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that you have compiled your training set, you will train three different classifiers with the same dataset: a discriminant function (SVM), a discriminative model (LR) and a generative model (NB), and compare the performance of all three in terms of training time and performance on the test set. In order to do that, you must import the three classifiers and train them, much like you did with the LR classifier in Activity 2. \n",
    "\n",
    "The three classifiers have already been constructed for you.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Train the three classifiers with the data that you collected in Activity 4. Report the time that each classifier took to train.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "# SVM model\n",
    "svm_model = LinearSVC()\n",
    "\n",
    "# Logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "t0 = time.time()\n",
    "svm_model.fit(X, a)\n",
    "svmT = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "lr_model.fit(X, a)\n",
    "lrT = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "nb_model.fit(X, a)\n",
    "nbT = time.time() - t0\n",
    "\n",
    "print(\"SVM model time: \", svmT)\n",
    "print(\"\\nLogistic Regression model time: \", lrT)\n",
    "print(\"\\nNaive Bays model time: \", nbT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, you will test the performance of the three classifiers in the test data. To that purpose, you must read the data in the `*-test` folders into a matrix of test points and the corresponding labels, and compare your predictions in this data with the actual labels. \n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "For the messages in the folders `*-test` compute the predictions of your classifiers. Then, use the function `confusion_matrix` (which has been imported for you) to analyze the performance of your method. Report the accuracy of each classifier (i.e., the percentage of correct answers) and comment on the advantages and disadvantages of the three methods for this task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "words = []\n",
    "\n",
    "files = []\n",
    "for training_dir in ['data\\\\spam-test', 'data\\\\nonspam-test']:\n",
    "    files += [os.path.join(training_dir, f) for f in os.listdir(training_dir)]\n",
    "\n",
    "X = np.zeros((len(files), len(wordVec)))\n",
    "\n",
    "#auxiliary\n",
    "print(\"Computing X...\")\n",
    "fileWords = []\n",
    "for i in range(len(files)):\n",
    "    fin = open(files[i], 'r')\n",
    "    fileWords += str(fin.read()).split()\n",
    "    \n",
    "    #dict with all the valid words\n",
    "    wordCounter = Counter(fileWords)\n",
    "    wordCounter = {k: v for k, v in wordCounter.items() if k.isalpha() and len(k)>1}\n",
    "    \n",
    "    #remove any word not in wordVec\n",
    "    toDelete = []\n",
    "    for k, v in wordCounter.items():\n",
    "        if k not in wordVec:\n",
    "            toDelete.append(k)\n",
    "    for w in toDelete:\n",
    "        wordCounter.pop(w)\n",
    "    \n",
    "    #filling each row\n",
    "    for j in range(len(wordVec)):\n",
    "        toDelete = \"\"\n",
    "        for k, v in wordCounter.items():\n",
    "            if k == wordVec[j]:\n",
    "                X[i, j] = v\n",
    "                toDelete = k\n",
    "                break\n",
    "        \n",
    "        #just to make things faster we delete the used keys\n",
    "        if (toDelete != \"\"):\n",
    "            wordCounter.pop(toDelete)\n",
    "    \n",
    "    fileWords = []\n",
    "    fin.close()\n",
    "    if (i % 100 == 0):\n",
    "        print(i, \"/\", len(files))\n",
    "\n",
    "print(\"X is computed\\n\")\n",
    "\n",
    "#a vector\n",
    "for i in range(len(files)):\n",
    "    if (files[i].split(\"\\\\\")[1] != 'spam-test'):\n",
    "        break\n",
    "a = np.append(np.zeros(i), np.ones(len(files) - i), axis = 0)\n",
    "\n",
    "t0 = time.time()\n",
    "svmPredict = svm_model.predict(X)\n",
    "svmPredictT = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "lrPredict = lr_model.predict(X)\n",
    "lrPredictT = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "nbPredict = nb_model.predict(X)\n",
    "nbPredictT = time.time() - t0\n",
    "\n",
    "numFiles = a.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(a, svmPredict)\n",
    "svmHits = cm[0,0] + cm[1,1]\n",
    "cm = confusion_matrix(a, lrPredict)\n",
    "lrHits = cm[0,0] + cm[1,1]\n",
    "cm = confusion_matrix(a, nbPredict)\n",
    "nbHits = cm[0,0] + cm[1,1]\n",
    "\n",
    "print(\"Analysis:\\n\")\n",
    "print(\"SVM fit time: \", svmT)\n",
    "print(\"SVM predict time: \", svmPredictT)\n",
    "print(\"SVM total time: \", svmT + svmPredictT)\n",
    "print(\"SVM performance: \", svmHits,\"/\", numFiles, \"(\", np.round(100*svmHits/numFiles, 2), \"%)\")\n",
    "\n",
    "print(\"\\n------\\n\")\n",
    "print(\"LR fit time: \", lrT)\n",
    "print(\"LR predict time: \", lrPredictT)\n",
    "print(\"LR total time: \", lrT + lrPredictT)\n",
    "print(\"LR performance: \", lrHits,\"/\", numFiles, \"(\", np.round(100*lrHits/numFiles, 2), \"%)\")\n",
    "\n",
    "print(\"\\n------\\n\")\n",
    "print(\"NB fit time: \", nbT)\n",
    "print(\"NB predict time: \", nbPredictT)\n",
    "print(\"NB total time: \", nbT + nbPredictT)\n",
    "print(\"NB performance: \", nbHits,\"/\", numFiles, \"(\", np.round(100*nbHits/numFiles, 2), \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"In terms of performance, NB is the worse of the three, missing 5 classifications. Both SVM and LR only fail 3.\")\n",
    "print(\"We divided the time analysis in two parts, the time needed to fit the data and the time needed to predict.\")\n",
    "print(\"NB is the model that takes less time to fit the data (by a big margin), followed by SVM and LR.\")\n",
    "print(\"LR is the model that takes less time to predict, followed by NB and SVM.\")\n",
    "print(\"NB takes significant less total time than the other two models. SVM is \", np.round(lrT + lrPredictT - svmT + svmPredictT, 3), \"s faster than LR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
